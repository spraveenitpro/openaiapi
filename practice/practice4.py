"""Context Explainer Factory using the Chat Completions API.

This module demonstrates a small yet production-style pattern where a factory
returns a callable that keeps shared configuration (model, tone, audience) and
invokes the OpenAI Chat Completions API. The combination of clear helper
functions, type hints, and docstrings keeps the code easy to understand and
reusable for other practice exercises.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Callable

from dotenv import load_dotenv
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage

# Load environment variables (e.g., OPENAI_API_KEY) once at import time.
load_dotenv()


@dataclass(frozen=True)
class ExplainerConfig:
    """Configuration bundle for the explainer factory."""

    model: str = "gpt-4o-mini"
    temperature: float = 0.4
    max_tokens: int | None = 256


def build_system_prompt(audience: str) -> str:
    """Create a system prompt tailored to the target audience."""

    return (
        "You are a patient instructor. "
        f"Adapt explanations to a {audience} audience, use concrete analogies, "
        "and finish with one actionable suggestion."
    )


def build_user_prompt(topic: str, context: str | None = None) -> str:
    """Create the user prompt that requests an explanation for the topic."""

    if context:
        return f"Explain the following topic in context of {context}: {topic}"
    return f"Explain the following topic in plain language: {topic}"


def make_chat_completion(
    client: OpenAI,
    system_prompt: str,
    user_prompt: str,
    *,
    config: ExplainerConfig,
) -> ChatCompletionMessage:
    """Call the Chat Completions API and return the first choice message."""

    response = client.chat.completions.create(
        model=config.model,
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    message = response.choices[0].message
    if not message.content:
        raise ValueError("Chat Completions API returned an empty message")
    return message


def explainer_factory(
    audience: str,
    *,
    client: OpenAI | None = None,
    config: ExplainerConfig | None = None,
) -> Callable[[str, str | None], str]:
    """Return a function that explains any topic for the requested audience.

    Args:
        audience: Target proficiency level (e.g., "beginner", "product manager").
        client: Optional pre-configured OpenAI client (mainly for testing).
        config: Optional configuration overrides for model/temperature.

    Returns:
        Callable that accepts `topic` and optional `context` strings and yields an
        explanation generated by the Chat Completions API.
    """

    resolved_client = client or OpenAI()
    resolved_config = config or ExplainerConfig()

    def explain(topic: str, context: str | None = None) -> str:
        """Explain `topic` for the captured audience and return the response."""

        if not topic.strip():
            raise ValueError("Topic cannot be empty")

        system_prompt = build_system_prompt(audience)
        user_prompt = build_user_prompt(topic, context)
        message = make_chat_completion(
            resolved_client,
            system_prompt,
            user_prompt,
            config=resolved_config,
        )
        return message.content.strip()

    return explain


def main() -> None:
    """Showcase the explainer factory with a couple of audiences."""

    beginner_explainer = explainer_factory("beginner")
    manager_explainer = explainer_factory("product manager")

    topic = "vector databases"

    print("Beginner explanation:\n")
    print(beginner_explainer(topic))

    print("\nProduct manager explanation with context:\n")
    print(manager_explainer(topic, context="personalization backlog"))


if __name__ == "__main__":
    main()
